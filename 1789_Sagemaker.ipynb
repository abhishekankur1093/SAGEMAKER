{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11666009",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2c0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\KIIT\\anaconda3\\Scripts\\huggingface-cli.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\KIIT\\anaconda3\\Lib\\site-packages\\huggingface_hub\\commands\\huggingface_cli.py\", line 49, in main\n",
      "    service.run()\n",
      "  File \"C:\\Users\\KIIT\\anaconda3\\Lib\\site-packages\\huggingface_hub\\commands\\user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"C:\\Users\\KIIT\\anaconda3\\Lib\\site-packages\\huggingface_hub\\_login.py\", line 110, in login\n",
      "    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)\n",
      "  File \"C:\\Users\\KIIT\\anaconda3\\Lib\\site-packages\\huggingface_hub\\_login.py\", line 306, in _login\n",
      "    raise ValueError(\"Invalid token passed!\")\n",
      "ValueError: Invalid token passed!\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token YOUR_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ba645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\KIIT\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must setup local AWS configuration with a region supported by SageMaker.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sess \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# sagemaker session bucket -> used for uploading data, models and logs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# sagemaker will automatically create this bucket if it not exists\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sagemaker_session_bucket\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sagemaker\\session.py:265\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket, settings, sagemaker_metrics_client, sagemaker_config, default_bucket_prefix)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m=\u001b[39m settings \u001b[38;5;28;01mif\u001b[39;00m settings \u001b[38;5;28;01melse\u001b[39;00m SessionSettings()\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(\n\u001b[0;32m    266\u001b[0m     boto_session\u001b[38;5;241m=\u001b[39mboto_session,\n\u001b[0;32m    267\u001b[0m     sagemaker_client\u001b[38;5;241m=\u001b[39msagemaker_client,\n\u001b[0;32m    268\u001b[0m     sagemaker_runtime_client\u001b[38;5;241m=\u001b[39msagemaker_runtime_client,\n\u001b[0;32m    269\u001b[0m     sagemaker_featurestore_runtime_client\u001b[38;5;241m=\u001b[39msagemaker_featurestore_runtime_client,\n\u001b[0;32m    270\u001b[0m     sagemaker_metrics_client\u001b[38;5;241m=\u001b[39msagemaker_metrics_client,\n\u001b[0;32m    271\u001b[0m     sagemaker_config\u001b[38;5;241m=\u001b[39msagemaker_config,\n\u001b[0;32m    272\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sagemaker\\session.py:292\u001b[0m, in \u001b[0;36mSession._initialize\u001b[1;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, sagemaker_metrics_client, sagemaker_config)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mregion_name\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust setup local AWS configuration with a region supported by SageMaker.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client \u001b[38;5;241m=\u001b[39m sagemaker_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    297\u001b[0m prepend_user_agent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client)\n",
      "\u001b[1;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "print(dataset[\"train\"][345][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/text-to-sql'\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'model_id': \"codellama/CodeLlama-7b-hf\",           # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': True,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'codellama-7b-hf-text-to-sql-exp1'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_sft.py',    # train script\n",
    "    source_dir           = '../scripts/trl',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                            # \"HF_TOKEN\": \"REPALCE_WITH_YOUR_TOKEN\" # huggingface token to access gated models, e.g. llama 2\n",
    "                            }, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c85e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d793de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[345]\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "print(random_sample[\"messages\"][1])\n",
    "request(random_sample[\"messages\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(sample):\n",
    "    predicted_answer = request(sample[\"messages\"][:2])\n",
    "    if predicted_answer[\"content\"] == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(test_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ced38ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[38;5;241m.\u001b[39mdelete_model()\n\u001b[0;32m      2\u001b[0m llm\u001b[38;5;241m.\u001b[39mdelete_endpoint()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af1266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
